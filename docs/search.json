[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports; example, math equation \\(^2 + b^2 = c^2\\).","code":""},{"path":"index.html","id":"usage","chapter":"1 About","heading":"1.1 Usage","text":"bookdown chapter .Rmd file, .Rmd file can contain one (one) chapter. chapter must start first-level heading: # good chapter, can contain one (one) first-level heading.Use second-level higher headings within chapters like: ## short section ### even shorter section.index.Rmd file required, also first book chapter. homepage render book.","code":""},{"path":"index.html","id":"render-book","chapter":"1 About","heading":"1.2 Render book","text":"can render HTML version example book without changing anything:Find Build pane RStudio IDE, andFind Build pane RStudio IDE, andClick Build Book, select output format, select “formats” ’d like use multiple formats book source files.Click Build Book, select output format, select “formats” ’d like use multiple formats book source files.build book R console:render example PDF bookdown::pdf_book, ’ll need install XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.org/tinytex/.","code":"\nbookdown::render_book()"},{"path":"index.html","id":"preview-book","chapter":"1 About","heading":"1.3 Preview book","text":"work, may start local server live preview HTML book. preview update edit book save individual .Rmd files. can start server work session using RStudio add-“Preview book”, R console:","code":"\nbookdown::serve_book()"},{"path":"journals.html","id":"journals","chapter":"2 Journals","heading":"2 Journals","text":"","code":""},{"path":"journals.html","id":"day-1","chapter":"2 Journals","heading":"2.1 Day 1","text":"class lectured January 18th learned quality department statistics KSU can impact professional lives future. department statistics provide us whole opportunity go deeper develop statistical skills. Also, main book recommended professor seems great support us throughout semester. Lastly, something hadn’t realized , basically data can considered spatio-temporal due fact collected certain time location.introductory class, nothing struggled understand.","code":""},{"path":"journals.html","id":"day-2","chapter":"2 Journals","heading":"2.2 Day 2","text":"class lectured January 25th mostly talked example spatio-temporal analysis simple dataset collected professor. tested three different models: ) 10th degree polynomial model; b) generalized additive model using non-linear, smooth relationship employing Gaussian process; c) decision tree. Besides , take away messages:size dataset limiting factor employ statistical analysis.size dataset limiting factor employ statistical analysis.research, unable collect data without error, imply adding another source variability analysis: error. happens sometimes don’t care dimension error, always .research, unable collect data without error, imply adding another source variability analysis: error. happens sometimes don’t care dimension error, always .distribution error crucial predictions want look interval distribution likely prediction value.distribution error crucial predictions want look interval distribution likely prediction value.Overfitting x underfitting complex discussion needs carefully interpreted.Overfitting x underfitting complex discussion needs carefully interpreted.Something kind struggled little bit need study distributions write model properly.","code":""},{"path":"journals.html","id":"day-3","chapter":"2 Journals","heading":"2.3 Day 3","text":"Questions covered class:• know model good/reliable?• improve model?• overfitting?Implementation process model truth location recorded location, hierarchical statistical methods come .space time statistics almost impossible replicate things. However, marathon example, one run using second/third GPS devices run marathon create replications.Data + assumptions = predictions/inferences/forecasts. assumptions mandatory!mob function r: instead flat lines regression tree, actual statistical model ‘line’.marathon example fit quadratic model.Spatio-temporal adopted Bayesian hierarchical models due fact traditional methods good expected. Linear model pretty good estimate good measure uncertainty.complex model usually best prediction wise good inference (happened speed marathon example). means sometimes depending question needs answered, one can overcomplicate things.","code":""},{"path":"journals.html","id":"day-4","chapter":"2 Journals","heading":"2.4 Day 4","text":"careful removing outliers making modifications data.Try avoid apply statistics statistics, rather model original raw data instead.Collect information help building model assumptions/changing .Prediction vs forecast. Prediction restricted range data . Forescast can go extrapolate range data. Hindcast follows rules forecast towards past instead future.Certain methods good prediction, machine learning methods. one try move forecasts hindcasts usually become good probably missing assumptions.Alternative save data test validate different models probably result less complex models.Things interested : predictions, forecast, hindcast statistical inferences. last one can observable unobservable (slope, example).Reliably quantify communicate uncertainty.Adjustment assumptions can help narrow prediction intervals.talk mathematical equations can picture real mean understand every piece formula. Looking normal distribution equation kind scares first moment tell much apply interpret . need go deeper .","code":""},{"path":"journals.html","id":"day-5","chapter":"2 Journals","heading":"2.5 Day 5","text":"variance (second moment) can also modeled.Expected value mean actually different things.Different distributions different moments.Least squares provide estimates parameters, able generate p value generate fake data.simply mathematical model.Moments distribution: expected value variance. calculate mean estimate expected value. However, even though mean expected can value, important highlight conceptually different things.might treat time either continuous(differential equations) discrete (difference equations), depending situation.don’t think fully understood concepts difference equations differential equations. understood first one treating time discrete values second one continuous values. However, formula application 100% clear.","code":""},{"path":"journals.html","id":"day-6","chapter":"2 Journals","heading":"2.6 Day 6","text":"Spatio-temporal data: data collected given time location. Almost data spatiotemporal, something happening given time space.Hierarchical models examples: mixed models, kriging, Bayesian models.Hierarchical models can divided : Empirical model Bayesian model. difference second one includes parameter model, whereas first one uses maximum likelihood.Framework Bayesian hierarchical model: Data model, process model parameter model (one present empirical framework). y data wish . Also known priors.assumes data collected error, probability distribution error. may small huge.Outputs: Posterior distribution parameters Posterior predictive distribution z column matrix observed data. y true value variable measured z priors.honest, don’t think doubt class, everything clear. just need study Bayesian.","code":""},{"path":"journals.html","id":"day-7","chapter":"2 Journals","heading":"2.7 Day 7","text":"Whooping cranes good example prediction inference.goal prediction, probably machine learning best choice.goal inference, probably regression anova can pretty much meet goals. want need find something .clarify want, goal.write model think suitable.google look packages/papers already used methodology.real meaning p data model unclear . familiar Bayesian\nstruggled little bit understand hierarchical sequence model think end \nunderstood, move forward make sense .\ntake notes \\({\\theta_D}\\) model description.","code":""},{"path":"journals.html","id":"day-8","chapter":"2 Journals","heading":"2.8 Day 8","text":"whooping crane example p =\\({\\theta_D}\\), means probability distribution.‘rejection’ parameters: simulated ‘fake’ data fits well model keep , doesn’t reject\n.result posterior prediction distribution yNot happy posterior distribution: either collect data adjust assumptions.understand real meaning blue line showed posterior distribution parameters. mean percentages? clear means.meant stay longer office hours appointment. mentioned class hard (almost impossible?!) measure uncertainty without using Bayesian. However, use traditional\napproach (let’s say linear model) bootstrap calculate confidence intervals? intervals uncertainty measurement?Regarding final project, joining someone Stat department mandatory just recommendation?","code":""},{"path":"journals.html","id":"day-9","chapter":"2 Journals","heading":"2.9 Day 9","text":"Example: extreme precipitation KansasGoal: mapping rainfall predict rainfall. Inference highest precipitation amount.Careful deleting missing data, might information change meaning analysis.Currently, sf package R reliable one spatial analysis manage shapefiles.talked methodologies deal missing data follow since aware methodologies deal missing data analysis.","code":""},{"path":"journals.html","id":"day-10","chapter":"2 Journals","heading":"2.10 Day 10","text":"Case Deletion missing data methodology assumption: NAs related response variable.Grabbing nearest weather station basically KNN approach, machine learning technique.Think check validate model /model selection.clear wrap class Gaussian processes, well multivariate normal distribution notation.","code":""},{"path":"journals.html","id":"day-11","chapter":"2 Journals","heading":"2.11 Day 11","text":"Gaussian process one useful distribution stat.distributions spatial points even contours.Gaussian assumes multivariate normal distribution.Correlation matrix - correlation covariates.Multivariate Normal Distribution need deal dependence.Random block effect assumption just like compound symmetry matrix.AR(1) dependence observations given time. Basically correlation matrix time-series data.ACF: dependence observed neighborsCorrelation function: mathematical function describes correlated random variables.correlation function d describes distance whereas ϕ describes ‘growth rate’.good class, however showed eta calculated correlation matrix AR(1) lines got confused differenciate .","code":""},{"path":"journals.html","id":"day-12","chapter":"2 Journals","heading":"2.12 Day 12","text":"model applied extreme precipitation example analysis 2 script, specifically model using gls function.spatial stats kind switch replication smoothness.struggled today understand gls model nugget term.","code":""},{"path":"journals.html","id":"day-13","chapter":"2 Journals","heading":"2.13 Day 13","text":"","code":""},{"path":"journals.html","id":"day-14","chapter":"2 Journals","heading":"2.14 Day 14","text":"starting workshop discussed alternatives explore elevation activity, kriging, regression tree gradient boosting machine learning methods.","code":""},{"path":"journals.html","id":"day-15","chapter":"2 Journals","heading":"2.15 Day 15","text":"Read chapter 6 technical note 1.1 pag 13 - Wikle et al. (2019).Memorization values data important analyzing.problem , assumption saying precipitation can negative infinite assumes normal distribution.Classic kriging doesn’t work large datasets.Statistical analysis 3 - kriging two error terms using low-rank approximation (aka modern kriging big data).model 3 2 using low-rank approximation.","code":""},{"path":"journals.html","id":"day-16","chapter":"2 Journals","heading":"2.16 Day 16","text":"Jacob discussing Tweedie distribution today fit model explain precipitation.struggled fit bayesian model using tweedie distribution. know possible brms package? just using Stan Jags?","code":""},{"path":"journals.html","id":"day-17","chapter":"2 Journals","heading":"2.17 Day 17","text":"Chapter 6 proposes way test ‘best’ distribution.MAE proper metric normal distribution data.cases calibration important prediction accuracy.prediction interval covering ~95% data points sign assumptions model problems.","code":""},{"path":"journals.html","id":"day-18","chapter":"2 Journals","heading":"2.18 Day 18","text":"Checking calibration model performance crucial good fitting.struggled understand Tweedie distribution model precipitation project. Especially coding part.","code":""},{"path":"journals.html","id":"day-19","chapter":"2 Journals","heading":"2.19 Day 19","text":"TutorialBird Cherry-oat Aphid dataset descriptionActivity 3 similar spatial prediction class example use land cover info . Check code.Poisson good distribution counting.","code":""},{"path":"journals.html","id":"day-20","chapter":"2 Journals","heading":"2.20 Day 20","text":"Regarding class project, Jacob discussed models fit data distributions use. reference model KNN model try fit hierarchical model using zero-inflated hurdle distributions model rainfall.discussed basis function used last class sure used exponential function. Also, tried understand models fitted Enders example, sure fully understood .","code":""},{"path":"journals.html","id":"day-21","chapter":"2 Journals","heading":"2.21 Day 21","text":"Activity 3Goal: accurately predict (map) abundance probability BYDV infection times locations data collected.make assumption collected data true data.exponential process model link function.example, ηs one distribution assumption behind since smothing s function. case assuming multivariate normal distribution.example interested second coefficient portraits increase aphids grassland percentage increases.second grassland % zero inflated poisson basically say far negative increase abundance likely zero.use AIC allows test best model without sacrifizing portion data model testing.Concurvity advanced approach multicollinearity.random effect penalizing model according concurvity can either ackowledge limitations model remove random effect.","code":""},{"path":"journals.html","id":"day-22","chapter":"2 Journals","heading":"2.22 Day 22","text":"","code":""},{"path":"journals.html","id":"day-23","chapter":"2 Journals","heading":"2.23 Day 23","text":"AIC: model checking don’t want sacrifize data testing.Concurvity similar colinearity. Measure covariate relates spatial covariate random effect.Variogram: check dependence residuals. Ideally now shape distribution points, following linear flat line.","code":""},{"path":"journals.html","id":"day-24","chapter":"2 Journals","heading":"2.24 Day 24","text":"Jacob worked project. discussed final adjustments analysis talked main goals writing.","code":""},{"path":"journals.html","id":"day-25","chapter":"2 Journals","heading":"2.25 Day 25","text":"Earthquake exampleLocation time random variables.y = zAssumptiom: measured data pretty close real data.","code":""},{"path":"journals.html","id":"day-26","chapter":"2 Journals","heading":"2.26 Day 26","text":"Jacob worked class project. discussed calibration uncertainty Aidan. struggling good calibration data realized data set hard achieve reasonable calibration. Regarding uncertainty, talked Aidan decided average daily uncertainty location predictions.","code":""},{"path":"journals.html","id":"day-27","chapter":"2 Journals","heading":"2.27 Day 27","text":"Uncertainty forecasts using GAM functions hugely wide.partial differential equations quite challenging.","code":""},{"path":"journals.html","id":"day-28","chapter":"2 Journals","heading":"2.28 Day 28","text":"Jacob way worked last adjustments final project sending peer review.","code":""},{"path":"activity-1.html","id":"activity-1","chapter":"3 Activity 1","heading":"3 Activity 1","text":"","code":"\n\nurl <- 'https://www.dropbox.com/scl/fi/2mufv5tlloz06k5ncwyx8/Afternoon_Walk.gpx?rlkey=6jzq31fonrs95glnfibi7lscp&dl=1'\n\nst_layers(url)\ndata <- st_read(url, layer = \"track_points\")"},{"path":"activity-1.html","id":"plotmap-your-movement-data.-i-would-recommend-using-r-andor-google-earth-as-i-demonstrated-in-class.","chapter":"3 Activity 1","heading":"3.1 Plot/map your movement data. I would recommend using R and/or Google earth as I demonstrated in class.","text":"","code":"\ncoords <- as.data.frame(st_coordinates(data))\ncoords$time <- data$time\n\nleaflet(coords) %>% \n  addTiles(group = \"OSM (default)\") %>% \n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") %>% \n  addCircleMarkers(~X, ~Y, color = ~time, radius = 2, fillOpacity = .8, group = \"Data Points\") %>% \n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"World Imagery\"),\n    overlayGroups = c(\"Data Points\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )"},{"path":"activity-1.html","id":"explore-your-movement-data.-for-example-are-there-any-unique-features-of-your-data-e.g.-a-large-change-in-location-do-your-data-contain-location-error-really-try-to-explore-your-data-as-best-as-possible-using-the-plotsmaps-you-made-in-3.","chapter":"3 Activity 1","heading":"3.2 Explore your movement data. For example, are there any unique features of your data (e.g., a large change in location)? Do your data contain location error? Really try to explore your data as best as possible using the plots/maps you made in 3.","text":"data actually pretty good. just problems beginning inside building probably carrier service good.","code":"\ntime <- as_datetime(coords$time) - as_datetime(coords$time)[1]\nele <- data$ele\n\ndf <- data.frame(long = coords[,1], \n                 lat = coords[,2],\n                 time = time,\n                 ele = ele)\n\ndf %>% pivot_longer(cols = c(long,lat,ele)) %>% \n  ggplot()+\n  geom_point(aes(time,value), size = 2, shape = 21, fill = 'steelblue', alpha = .8)+\n  facet_wrap(~name, scales = 'free')"},{"path":"activity-1.html","id":"fit-a-statistical-or-machine-learning-model-to-your-movement-data.-obtain-predictions-of-your-location-on-a-fine-time-scale-so-that-the-estimates-resemble-a-continuous-trajectory.","chapter":"3 Activity 1","heading":"3.3 Fit a statistical or machine learning model to your movement data. Obtain predictions of your location on a fine time scale so that the estimates resemble a continuous trajectory.","text":"","code":"\n# Fit models polynomial regression and random forest.\n\n# Longitude\nm1_long <- lm(long ~ poly(time,degree=10,raw=TRUE),data=df)\nsummary(m1_long)\nm2_long <- randomForest(long ~ time, data = df)\nsummary(m2_long)\n\n# Latitude\nm1_lat <- lm(lat ~ poly(time,degree=10,raw=TRUE),data=df)\nsummary(m1_lat)\nm2_lat <- randomForest(lat ~ time, data = df)\nsummary(m2_lat)\n\n# Elevation\nm1_ele <- lm(ele ~ poly(time,degree=10,raw=TRUE),data=df)\nsummary(m1_lat)\nm2_ele <- randomForest(ele ~ time, data = df)\nsummary(m2_lat)"},{"path":"activity-1.html","id":"plotmap-your-estimated-trajectory-from-5.-explore-your-estimated-trajectory-as-best-as-possible-using-the-plotsmaps.-note-any-unique-features-or-shortcomings-of-your-model.","chapter":"3 Activity 1","heading":"3.4 Plot/map your estimated trajectory from 5. Explore your estimated trajectory as best as possible using the plots/maps. Note any unique features or shortcomings of your model.","text":"","code":"\ndf.pred = data.frame(time = seq(0,as.integer(max(df$time))))\n\ndf.pred$long.m1.hat = predict(m1_long, newdata = df.pred)\ndf.pred$long.m2.hat = predict(m2_long, newdata = df.pred)\n\ndf.pred$lat.m1.hat = predict(m1_lat, newdata = df.pred)\ndf.pred$lat.m2.hat = predict(m2_lat, newdata = df.pred)\n\ndf.pred$ele.m1.hat = predict(m1_ele, newdata = df.pred)\ndf.pred$ele.m2.hat = predict(m2_ele, newdata = df.pred)\n\np1 <- ggplot()+\n  geom_point(data = df, aes(time, long), size = 3)+\n  geom_line(data = df.pred, aes(time, long.m1.hat), color = \"gold\", size = 1)+\n  geom_line(data = df.pred, aes(time, long.m2.hat), color = \"red4\", size = 1)\n\np2 <- ggplot()+\n  geom_point(data = df, aes(time, lat), size = 3)+\n  geom_line(data = df.pred, aes(time, lat.m1.hat), color = \"gold\", size = 1)+\n  geom_line(data = df.pred, aes(time, lat.m2.hat), color = \"red4\", size = 1)\n\np3 <- ggplot()+\n  geom_point(data = df, aes(time, ele), size = 3)+\n  geom_line(data = df.pred, aes(time, ele.m1.hat), color = \"gold\", size = 1)+\n  geom_line(data = df.pred, aes(time, ele.m2.hat), color = \"red4\", size = 1)\n\n\nggarrange(p1, p2, p3, nrow = 1)\n# Create data frame for plotting\ndf.pred2 <- df.pred %>% pivot_longer(cols = c(long.m1.hat,long.m2.hat), values_to = 'longitude', names_to = 'model') %>% \n  pivot_longer(cols = c(lat.m1.hat, lat.m2.hat), values_to = 'latitude', names_to = 'model2') %>% \n  mutate(model = substr(model, 6,7),\n         model2 = substr(model2, 5,6)) %>% \n  filter(model == model2) %>% dplyr::select(-c(ele.m1.hat, ele.m2.hat, model2))\n\n# Visualize models\n\ncolor_palette <- colorFactor(palette = \"Set1\", domain = df.pred2$model)\n\nleaflet(df.pred2) %>%\n  addTiles(group = \"OSM (default)\") %>%\n  addProviderTiles(providers$Esri.WorldImagery, group = \"World Imagery\") %>%\n  addCircleMarkers(\n    ~longitude,\n    ~latitude,\n    fillColor = ~color_palette(model), \n    color = ~'black', # This will set the border color the same as the fill color\n    radius = 3,\n    weight = 1,\n    fillOpacity = 0.8,\n    stroke = TRUE, # Set to TRUE to have borders on the circles\n    group = \"Data Points\"\n  ) %>%\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"World Imagery\"),\n    overlayGroups = c(\"Data Points\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>%\n  addLegend(\n    position = \"bottomright\",\n    pal = color_palette,\n    values = ~model,\n    title = \"Model\",\n    opacity = 1.0\n  )"},{"path":"activity-1.html","id":"estimate-a-feature-or-quantity-of-interest-from-your-estimated-trajectory-e.g.-velocity-residence-time-number-of-contacts-etc","chapter":"3 Activity 1","heading":"3.5 Estimate a feature or quantity of interest from your estimated trajectory (e.g., velocity, residence time, number of contacts, etc)","text":"","code":"\n# Calculate speed observed data\ndist <- st_distance(data$geometry[1:701], data$geometry[2:702], by_element = T)\n(sum(dist)/1000)*.62 # Distance observed in km\n#> 2.491147 [m]\nspeed <- (dist/as.numeric(diff(data$time)))*2.24\nplot(df$time[-1], speed)\n\n#Convert model coordinates to sf object\ndata.hat.m1 <- st_as_sf(df.pred, coords = c(\"long.m1.hat\", \"lat.m1.hat\"), \n                           crs = st_crs(data))\n\ndata.hat.m2 <- st_as_sf(df.pred, coords = c(\"long.m2.hat\", \"lat.m2.hat\"), \n                           crs = st_crs(data))\n\n# Calculate speed m1\ndist.hat.m1 <- st_distance(data.hat.m1$geometry[1:741], data.hat.m1$geometry[2:742], by_element = T)\n(sum(dist.hat.m1)/1000)*.62 # Distance in km model 1\n#> 2.576969 [m]\nspeed.hat.m1 <- (dist.hat.m1/as.numeric(diff(data.hat.m1$time)))*2.24\nplot(data.hat.m1$time[-1], speed.hat.m1,xlab=\"Time (seconds)\",ylab=\"Velocity (miles per hour)\", main = 'Polynomial regression')\n\n# Calculate speed m2\ndist.hat.m2 <- st_distance(data.hat.m2$geometry[1:741], data.hat.m2$geometry[2:742], by_element = T)\n(sum(dist.hat.m2)/1000)*.62 # Distance in km model 2\n#> 2.436481 [m]\nspeed.hat.m2 <- (dist.hat.m2/as.numeric(diff(data.hat.m2$time)))*2.24\nplot(data.hat.m2$time[-1], speed.hat.m2,xlab=\"Time (seconds)\",ylab=\"Velocity (miles per hour)\", main = 'Random Forest')"},{"path":"activity-2.html","id":"activity-2","chapter":"4 Activity 2","heading":"4 Activity 2","text":"","code":""},{"path":"activity-2.html","id":"chose-an-area-on-or-close-to-campus-where-it-is-easy-for-you-to-understand-how-the-elevation-changes.-for-example-i-chose-the-parking-lot-outside-of-dickens-hall.-using-a-smartphone-record-the-elevation-at-several-locations-points-within-the-area-you-chose.-i-recommend-using-the-app-strava-but-you-can-use-whatever-you-want.","chapter":"4 Activity 2","heading":"4.1 Chose an area on or close to campus where it is easy for you to understand how the elevation changes. For example, I chose the parking lot outside of Dickens Hall. Using a smartphone record the elevation at several locations (points) within the area you chose. I recommend using the app Strava, but you can use whatever you want.","text":"decided use harvest map retrieved combine dad’s farm. used operate combine know field well. believe meet requirements. Also, required information available, coordinates, elevation time.","code":""},{"path":"activity-2.html","id":"obtain-a-.gpx-or-.csv-file-for-your-elevation-data.-at-minimum-the-file-should-contain-the-location-and-time-of-the-elevation-measurements.","chapter":"4 Activity 2","heading":"4.2 Obtain a .gpx or .csv file for your elevation data. At minimum the file should contain the location and time of the elevation measurements.","text":"Upload data","code":"\n# Points\npoints <-  st_read('https://www.dropbox.com/scl/fi/5km5t8yzjqh9fltq5wz2f/soybean23map.geojson?rlkey=7k2ppf8hl9v4oq4nxvx6n2ket&st=xhmxfx42&dl=1') %>% \n  dplyr::select(Elevation, geometry, Time) %>% \n  .[sample(nrow(.), 100), ] \n#> Reading layer `OGRGeoJSON' from data source \n#>   `https://www.dropbox.com/scl/fi/5km5t8yzjqh9fltq5wz2f/soybean23map.geojson?rlkey=7k2ppf8hl9v4oq4nxvx6n2ket&st=xhmxfx42&dl=1' \n#>   using driver `GeoJSON'\n#> Simple feature collection with 25601 features and 13 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 314352.5 ymin: 6696958 xmax: 314835.8 ymax: 6697787\n#> Projected CRS: WGS 84 / UTM zone 22S\n# There are thousands of points, so for the purpose of this activity only a few will be utilized randomly.\n\n# Polygon\npolygon <- st_read('https://www.dropbox.com/scl/fi/bxbwxmgs22yx17j2g8m3y/pol.geojson?rlkey=lx39y99fewf8qgzfh87p0ptmp&st=ngala0hj&dl=1')\n#> Reading layer `OGRGeoJSON' from data source \n#>   `https://www.dropbox.com/scl/fi/bxbwxmgs22yx17j2g8m3y/pol.geojson?rlkey=lx39y99fewf8qgzfh87p0ptmp&st=ngala0hj&dl=1' \n#>   using driver `GeoJSON'\n#> Simple feature collection with 1 feature and 3 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -5891219 ymin: -3483519 xmax: -5890641 ymax: -3482545\n#> Projected CRS: WGS 84 / Pseudo-Mercator"},{"path":"activity-2.html","id":"plotmap-your-elevation-data.-i-would-recommend-using-r-andor-google-earth.","chapter":"4 Activity 2","heading":"4.3 Plot/map your elevation data. I would recommend using R and/or Google earth.","text":"","code":"\nggplot()+\n  geom_sf(data = polygon)+\n  geom_sf(data = points, aes(fill = Elevation), shape = 21)+\n  scale_fill_viridis_c()+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))"},{"path":"activity-2.html","id":"explore-your-elevation-data.-for-example-are-there-any-unique-features-of-your-data-do-your-data-contain-obvious-measurement-error-e.g.-an-elevation-that-cant-possibly-be-true-really-try-to-explore-your-data-as-best-as-possible-using-the-plotsmaps-you-made-in-.","chapter":"4 Activity 2","heading":"4.4 Explore your elevation data. For example, are there any unique features of your data? Do your data contain obvious measurement error (e.g., an elevation that can’t possibly be true)? Really try to explore your data as best as possible using the plots/maps you made in .","text":"","code":"\nggplot()+\n  geom_histogram(data = points, aes(x = Elevation), \n                 bins = 25, \n                 color = 'black',\n                 fill = 'tomato4',\n                 alpha = .5)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"activity-2.html","id":"write-out-the-goals-that-you-wish-to-accomplish-using-your-elevation-data.-for-example-my-goal-was-to-make-a-map-of-the-dickens-hall-parking-lot.-this-involves-using-the-elevation-data-i-collected-to-make-predictions-of-the-elevation-at-any-possible-spatial-locations-within-the-parking-lot.-i-would-also-like-to-make-inference-about-the-location-where-the-elevation-is-lowest-within-the-parking-lot.","chapter":"4 Activity 2","heading":"4.5 Write out the goals that you wish to accomplish using your elevation data. For example, my goal was to make a map of the Dicken’s Hall parking lot. This involves using the elevation data I collected to make predictions of the elevation at any possible spatial locations within the parking lot. I would also like to make inference about the location where the elevation is lowest within the parking lot.","text":"goal make predictions elevation location field also make inference elevation highest.","code":""},{"path":"activity-2.html","id":"write-out-several-statistical-or-machine-learning-models-that-you-think-you-can-use-to-answer-the-questionsgoals-you-wrote-in-prompt-5.-be-as-creative-and-inclusive-here.-for-each-statistical-or-machine-learning-model-make-sure-you-explain-each-component-piece-of-the-model","chapter":"4 Activity 2","heading":"4.6 Write out several statistical or machine learning models that you think you can use to answer the questions/goals you wrote in prompt #5. Be as creative and inclusive here. For each statistical or machine learning model, make sure you explain each component (piece) of the model","text":"Model 1 - Random Forest\\(\\hat{y}(\\textbf{X}) = \\frac{1}{B} \\sum_{b=1}^B T_b (\\textbf{X})\\)\\(B\\): number trees forest.\\(T_b(\\textbf{X})\\): prediction b-th tree input \\(X\\).\\(\\hat{y}(\\textbf{X})\\): final prediction random forest input \\(X\\).Model 2 - KrigingData Model\\(Y(s_i) = m(s_i) + \\epsilon(s_i)\\)\\(\\epsilon(s_i) \\sim N(0, \\sigma^2)\\)Process Model\\(\\hat{Y}(s_0) \\sim m(s_0) + \\sum_{=1}^{n} \\lambda_i [Y(s_i) - m(s_i)]\\)\\(\\hat{Y}(s_0)\\) represents predicted elevation new location \\(s_0\\).\\(m(s_0)\\) estimated mean trend new location \\(s_0\\).\\(Y(s_i)\\) observed elevations known locations \\(s_i\\).\\({\\lambda}_i\\) weights calculated minimize variance prediction error, based spatial autocorrelation structure.\\(m(s_i)\\) mean trend observed locations.\\(n\\) number observed locations used predictions.","code":""},{"path":"activity-2.html","id":"of-the-models-you-developed-in-prompt-6-find-or-develop-software-to-fit-at-least-two-of-these-models-to-your-elevation-data.-note-that-in-a-perfect-world-you-would-be-able-to-either-find-existing-software-or-develop-new-programs-that-enable-you-to-fit-any-statistical-or-machine-learning-model-you-want.-in-reality-you-may-may-end-up-having-to-make-some-unwanted-changes-to-your-models-in-prompt-6-to-be-able-to-find-existing-software-to-fit-these-models-to-the-data.","chapter":"4 Activity 2","heading":"4.7 7). Of the models you developed in prompt #6, find (or develop) software to fit at least two of these models to your elevation data. Note that in a perfect world, you would be able to either find existing software or develop new programs that enable you to fit any statistical or machine learning model you want. In reality, you may may end up having to make some unwanted changes to your models in prompt #6 to be able to find existing software to fit these models to the data.","text":"KrigingRandom Forest","code":"\n\n# Create random points\nnewPoints <- st_sample(polygon, size = 10000, type = \"random\") %>% \n  as(., 'Spatial') %>% \n  spTransform(., CRS(proj4string(points %>% as(.,'Spatial'))))\npoints\n#> Simple feature collection with 100 features and 2 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 314356.1 ymin: 6696981 xmax: 314828.5 ymax: 6697785\n#> Projected CRS: WGS 84 / UTM zone 22S\n#> First 10 features:\n#>       Elevation                Time\n#> 18847  46.69362 4/8/2023 7:40:08 PM\n#> 18895  48.92382 4/8/2023 7:40:56 PM\n#> 25102  61.36804 4/8/2023 9:41:34 PM\n#> 2986   66.15939 4/8/2023 2:30:08 PM\n#> 1842   50.01868 4/8/2023 2:04:36 PM\n#> 3371   50.05362 4/8/2023 2:36:33 PM\n#> 11638  71.28713 4/8/2023 5:14:48 PM\n#> 4761   60.44254 4/8/2023 3:03:38 PM\n#> 6746   58.86041 4/8/2023 3:42:34 PM\n#> 16128  43.89501 4/8/2023 6:46:11 PM\n#>                       geometry\n#> 18847 POINT (314620.2 6697715)\n#> 18895 POINT (314630.1 6697657)\n#> 25102 POINT (314639.5 6697339)\n#> 2986  POINT (314557.5 6697202)\n#> 1842  POINT (314484.4 6697654)\n#> 3371  POINT (314492.5 6697664)\n#> 11638   POINT (314446 6697021)\n#> 4761  POINT (314512.1 6697424)\n#> 6746    POINT (314489 6697471)\n#> 16128 POINT (314655.1 6697785)\n\nkrig.df <- data.frame(ele = points$Elevation,\n                      lon = st_coordinates(points$geometry)[,1],\n                      lat = st_coordinates(points$geometry)[,2])\n\nkrig.mod <- gam(ele ~ s(lon,lat, bs = 'gp'), data = krig.df)\n\nnewpoints.krig <-  as.data.frame(newPoints) %>% \n  rename(\"lon\" = 'coords.x1', \n         'lat' = 'coords.x2')\n\nnewpoints.krig$ele <- predict(krig.mod, newpoints.krig, type = 'response')\ndf.rf <- as.data.frame(points)\ndf.rf$lon <- st_coordinates(points)[,1]\ndf.rf$lat <- st_coordinates(points)[,2]\n\nrf.fit <- randomForest(Elevation ~ lon + lat, data=df.rf, ntree=500, importance=TRUE)\n\nplot(rf.fit)\n\nnewPoints.rf <- as.data.frame(newPoints) %>% \n  rename(\"lon\" = 'coords.x1', \n         'lat' = 'coords.x2')\n\npred.rf <- predict(rf.fit, newPoints.rf) %>% as.data.frame()\n\nnewPoints.rf$ele <- pred.rf$."},{"path":"activity-2.html","id":"related-to-prompt-5-use-both-models-you-fit-to-your-elevation-data-in-prompt-7-to-answer-the-questionsgoals.-for-my-elevation-data-this-would-include-making-a-predictive-heatmap-showing-the-elevation-of-the-dickens-hall-parking-lot-and-then-estimating-the-coordinates-of-the-point-where-elevation-is-at-a-minimum.","chapter":"4 Activity 2","heading":"4.8 Related to prompt #5, use both models you fit to your elevation data in prompt #7 to answer the questions/goals. For my elevation data, this would include making a predictive heatmap showing the elevation of the Dickens Hall parking lot and then estimating the coordinates of the point where elevation is at a minimum.","text":"KrigingRandom Forest","code":"\nkrig.sf <- st_as_sf(newpoints.krig, coords = c('lon','lat'), crs = st_crs(points))\n\nggplot()+\n  geom_sf(data = krig.sf, aes(fill = ele), \n          shape = 21)+\n    geom_sf(data = polygon, fill = NA, color = 'black')+\n  geom_sf(data = krig.sf %>% filter(ele == max(ele)), fill = 'darkred', shape = 22,\n          size = 3)+\n  scale_fill_viridis_c()+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))\nrf.sf <- st_as_sf(newPoints.rf, coords = c('lon','lat'), crs = st_crs(points))\n\nggplot()+\n  geom_sf(data = rf.sf, aes(fill = ele), \n          shape = 21)+\n    geom_sf(data = polygon, fill = NA, color = 'black')+\n  geom_sf(data = krig.sf %>% filter(ele == max(ele)), fill = 'darkred', shape = 22,\n          size = 3)+\n  scale_fill_viridis_c()+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))"},{"path":"activity-2.html","id":"based-on-the-material-in-chapter-6-of-spatio-temporal-statistics-with-r-and-our-discussion-in-class-on-march-26-compare-check-and-evaluate-the-two-models-from-8.","chapter":"4 Activity 2","heading":"4.9 Based on the material in Chapter 6 of Spatio-Temporal Statistics with R and our discussion in class on March 26, compare, check and evaluate the two models from #8.","text":"","code":"\n# Obtain new points at the same area for model testing.\nnewpoints.test <- st_read('https://www.dropbox.com/scl/fi/5km5t8yzjqh9fltq5wz2f/soybean23map.geojson?rlkey=7k2ppf8hl9v4oq4nxvx6n2ket&st=xhmxfx42&dl=1') %>% \n  dplyr::select(Elevation, geometry, Time) %>% \n  .[sample(nrow(.), 100), ]\n#> Reading layer `OGRGeoJSON' from data source \n#>   `https://www.dropbox.com/scl/fi/5km5t8yzjqh9fltq5wz2f/soybean23map.geojson?rlkey=7k2ppf8hl9v4oq4nxvx6n2ket&st=xhmxfx42&dl=1' \n#>   using driver `GeoJSON'\n#> Simple feature collection with 25601 features and 13 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 314352.5 ymin: 6696958 xmax: 314835.8 ymax: 6697787\n#> Projected CRS: WGS 84 / UTM zone 22S\n\n# Plot training and testing datasets\nggplot()+\n  geom_sf(data = polygon)+\n  geom_sf(data = points, shape = 21, fill = 'gold')+\n  geom_sf(data = newpoints.test, shape = 22, fill = 'pink4')+\n  scale_fill_viridis_c()+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))\n# Predictions\ntest.df <- data.frame(ele = newpoints.test$Elevation,\n                      lon = st_coordinates(newpoints.test$geometry)[,1],\n                      lat = st_coordinates(newpoints.test$geometry)[,2])\n\ntest.df$pred.krig <- predict(krig.mod, newdata = test.df, type = 'response')\ntest.df$pred.rf <- predict(rf.fit, newdata = test.df)\nrmse.krig <- rmse(test.df$ele, as.numeric(test.df$pred.krig))\nmae.krig <- mae(test.df$ele, as.numeric(test.df$pred.krig))\n\nrmse.rf <- rmse(test.df$ele, as.numeric(test.df$pred.rf))\nmae.rf <- mae(test.df$ele, as.numeric(test.df$pred.rf))\n\n#Kriging metrics\n\nkrigingMetrics <- test.df %>% \n  ggplot()+\n  geom_point(aes(pred.krig, ele), fill = 'purple4', color = 'black', shape = 21, size = 2,\n             alpha = .7)+\n  geom_abline(slope = 1)+\n  #scale_y_continuous(limits = c(0,90), breaks = seq(0,100, 20))+\n  #scale_x_continuous(limits = c(0,90), breaks = seq(0,100, 20))+\n  theme_bw()+\n  labs(title = 'Kriging', x = 'Predicted', y = 'Observed')+\n  annotate('text', label = paste0('RMSE: ', round(rmse.krig,1)), x = 65, y = 50)+\n  annotate('text', label = paste0('MAE: ', round(mae.krig,1)), x = 65, y = 48)+\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1,\n        text = element_text(size = 12)\n        )\n\n#Rf metrics\n\nrfMetrics <- test.df %>% \n  ggplot()+\n  geom_point(aes(pred.rf, ele), fill = 'purple4', color = 'black', shape = 21, size = 2,\n             alpha = .7)+\n  geom_abline(slope = 1)+\n  #scale_y_continuous(limits = c(0,90), breaks = seq(0,100, 20))+\n  #scale_x_continuous(limits = c(0,90), breaks = seq(0,100, 20))+\n  theme_bw()+\n  labs(title = 'Random Forest', x = 'Predicted', y = 'Observed')+\n  annotate('text', label = paste0('RMSE: ', round(rmse.rf,1)), x = 65, y = 50)+\n  annotate('text', label = paste0('MAE: ', round(mae.rf,1)), x = 65, y = 48)+\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1,\n        text = element_text(size = 12)\n        )\n\nggarrange(krigingMetrics,rfMetrics)"},{"path":"activity-3.html","id":"activity-3","chapter":"5 Activity 3","heading":"5 Activity 3","text":"","code":""},{"path":"activity-3.html","id":"upload-data","chapter":"5 Activity 3","heading":"5.1 Upload data","text":"","code":"\n# Upload data\nurl <- \"https://www.dropbox.com/scl/fi/9ymxt900s77uq50ca6dgc/Enders-et-al.-2018-data.csv?rlkey=0rxjwleenhgu0gvzow5p0x9xf&dl=1\"\ndf <- read.csv(url)\ndf <- df[,c(2,8:10)] %>% \n  mutate(presence = ifelse(EGA != 0, 1, 0))# Keep only the data on bird cherry-oat aphid\n\n# Download KS shapefile\nks <- raster::getData(name=\"GADM\", country=\"USA\", level=1) %>% \n  st_as_sf() %>% \n  filter(NAME_1 == 'Kansas')\n\ndf_sf <- df %>% st_as_sf(coords = c('long', 'lat'), crs = st_crs(ks))"},{"path":"activity-3.html","id":"upload-covariates","chapter":"5 Activity 3","heading":"5.2 Upload covariates","text":"","code":"\nurl.nlcd <- \"https://www.dropbox.com/scl/fi/ew7yzm93aes7l8l37cn65/KS_2011_NLCD.img?rlkey=60ahyvxhq18gt0yr47tuq5fig&dl=1\"\nrl.nlcd2011 <- raster(url.nlcd)\n\nplot(rl.nlcd2011)\n\n\n# Make raster file that contains pixels with value of 1 if grassland and \n# zero if other type of land cover.\n# NLCD legend can be found here: https://www.mrlc.gov/data/legends/national-land-cover-database-2011-nlcd2011-legend\nrl.nlcd.grass <- rl.nlcd2011\nrl.nlcd.grass[] <- ifelse(rl.nlcd.grass[]==71,1,0)\n\nplot(rl.nlcd.grass)\n\npts.sample<- df\n\ncoordinates(pts.sample) =~ long + lat\nproj4string(pts.sample) <- CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\")\n\n# Calculate percentage of land area that is grassland within 5 km of sampled location\ndf$grass.perc <- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100\n\nhist(df$grass.perc,col=\"grey\",main=\"\",xlab=\"% grassland within \\n5 km at sample location\")"},{"path":"activity-3.html","id":"for-the-data-on-the-abundance-of-english-grain-aphids-propose-three-different-statistical-models-or-machine-learning-approach-that-are-capable-of-predicting-the-number-of-english-grain-aphids-at-any-location-within-the-state-of-kansas-at-any-time-for-the-years-2014-and-2015.-make-sure-to-write-out-the-three-statistical-models-using-formal-notation-and-fully-describe-each-component-using-words.","chapter":"5 Activity 3","heading":"5.3 For the data on the abundance of English grain aphids, propose three different statistical models (or machine learning approach) that are capable of predicting the number of English grain aphids at any location within the state of Kansas at any time for the years 2014 and 2015. Make sure to write out the three statistical models using formal notation and fully describe each component using words.","text":"Model 1Data model \\[Z = y\\] Process model Using Poisson distribution\\[[y|\\lambda] = Poisson(\\lambda) \\]\\[\\eta_s\\sim MUN(0, \\sigma)\\] \\[\\eta_t\\sim MVN(0, \\sigma)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X~\\eta_s+\\eta_t}\\]Model 2Data model\\[Z = y\\] Process model Using Negative binomial distribution\\[[y|r,p] = NB(r, p)\\] \\[\\eta_s\\sim MVN(0, \\sigma)\\]\n\\[\\eta_t\\sim  MVN(0, \\sigma)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X~\\eta_s+\\eta_t}\\]Model 3Data model\\[Z = y\\] Process model Using zero inflated poisson distribution\\[[y|p, \\lambda]=ZIP(p,\\lambda)\\] \\[\\eta_s\\sim MVN(0, \\sigma^2)\\]\n\\[E(y)=e^{\\beta_0+\\beta_1\\cdot X~\\eta_s+\\eta_t}\\]","code":""},{"path":"activity-3.html","id":"for-the-three-statistical-models-you-proposed-in-question-1-propose-a-way-to-measure-the-accuracy-and-perhaps-the-calibration-of-predictions.","chapter":"5 Activity 3","heading":"5.4 For the three statistical models you proposed in question #1, propose a way to measure the accuracy (and perhaps the calibration) of predictions.","text":"RMSE MAE AIC.","code":""},{"path":"activity-3.html","id":"fit-the-three-statistical-models-you-proposed-in-question-1-to-the-english-grain-aphid-abundance-data.","chapter":"5 Activity 3","heading":"5.5 Fit the three statistical models you proposed in question #1 to the English grain aphid abundance data.","text":"","code":"\n\nggplot()+\n  geom_sf(data = ks)+\n  geom_sf(data = df_sf, shape = 21, aes(size = EGA, fill = factor(presence)))+\n  theme_bw()+\n  facet_wrap(~year, ncol = 1)"},{"path":"activity-3.html","id":"model-1","chapter":"5 Activity 3","heading":"5.5.1 Model 1","text":"","code":"\nm1 <- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), \n          family = poisson(link = \"log\"), data = df)\n\nsummary(m1)\n#> \n#> Family: poisson \n#> Link function: log \n#> \n#> Formula:\n#> EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n#> \n#> Parametric coefficients:\n#>                       Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)         -3.1624342  0.2629681 -12.026   <2e-16\n#> grass.perc          -0.0084544  0.0009811  -8.617   <2e-16\n#> as.factor(year)2015  5.5241770  0.2584600  21.373   <2e-16\n#>                        \n#> (Intercept)         ***\n#> grass.perc          ***\n#> as.factor(year)2015 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>               edf Ref.df Chi.sq p-value    \n#> s(long,lat) 31.97     32   8009  <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.395   Deviance explained = 69.9%\n#> UBRE = 28.333  Scale est. = 1         n = 341"},{"path":"activity-3.html","id":"model-2","chapter":"5 Activity 3","heading":"5.5.2 Model 2","text":"","code":"\nm2 <- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), \n          family = nb(theta = NULL,link = \"log\"), data = df)\n\nsummary(m2)\n#> \n#> Family: Negative Binomial(0.623) \n#> Link function: log \n#> \n#> Formula:\n#> EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n#> \n#> Parametric coefficients:\n#>                      Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)         -2.512753   0.343910  -7.306 2.74e-13\n#> grass.perc          -0.005170   0.004665  -1.108    0.268\n#> as.factor(year)2015  5.164253   0.325909  15.846  < 2e-16\n#>                        \n#> (Intercept)         ***\n#> grass.perc             \n#> as.factor(year)2015 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>               edf Ref.df Chi.sq p-value    \n#> s(long,lat) 8.884  11.75  372.1  <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.247   Deviance explained = 68.6%\n#> -REML = 962.32  Scale est. = 1         n = 341"},{"path":"activity-3.html","id":"model-3","chapter":"5 Activity 3","heading":"5.5.3 Model 3","text":"","code":"\n\nm3 <- gam(list(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = \"gp\"), ~ grass.perc + s(long,lat, bs = \"gp\")), \n          family = ziplss(), data = df)\n\nsummary(m3)\n#> \n#> Family: ziplss \n#> Link function: identity identity \n#> \n#> Formula:\n#> EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = \"gp\")\n#> ~grass.perc + s(long, lat, bs = \"gp\")\n#> \n#> Parametric coefficients:\n#>                       Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)         -1.5217298  0.3565564  -4.268 1.97e-05\n#> grass.perc          -0.0098653  0.0009923  -9.942  < 2e-16\n#> as.factor(year)2015  4.0446085  0.3513845  11.510  < 2e-16\n#> (Intercept).1       -0.0430138  0.1513463  -0.284    0.776\n#> grass.perc.1         0.0026990  0.0044434   0.607    0.544\n#>                        \n#> (Intercept)         ***\n#> grass.perc          ***\n#> as.factor(year)2015 ***\n#> (Intercept).1          \n#> grass.perc.1           \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>                 edf Ref.df  Chi.sq  p-value    \n#> s(long,lat)   31.55  31.84 6583.32  < 2e-16 ***\n#> s.1(long,lat) 11.98  15.64   43.45 0.000195 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Deviance explained = 61.3%\n#> -REML = 5369.7  Scale est. = 1         n = 341"},{"path":"activity-3.html","id":"create-points-for-prediction","chapter":"5 Activity 3","heading":"5.6 Create points for prediction","text":"","code":"\n####\nrl.E.y <- raster(,nrow=30,ncols=30,ext=extent(ks),crs=crs(ks))\nnewPoints <- data.frame(long = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,1],\n                      lat = xyFromCell(rl.E.y,cell=1:length(rl.E.y[]))[,2]) %>%\n  st_as_sf(coords = c('long', 'lat'), crs = st_crs(ks)) %>% \n  st_filter(ks) %>% as.data.frame() %>% \n  cross_join(data.frame(year = as.factor(c('2014', '2015'))))\n\nnewPoints$lat <- st_coordinates(newPoints$geometry)[,2]\nnewPoints$long <- st_coordinates(newPoints$geometry)[,1]\n\n# \n# newPoints <- st_sample(ks, size = 1000, type = \"regular\") %>%\n#   as(., 'Spatial') %>% as.data.frame() %>%\n#     rename(\"long\" = 'coords.x1',\n#          'lat' = 'coords.x2') %>%\n#   cross_join(data.frame(year = as.factor(c('2014', '2015'))))\n\n    \npts.sample<- newPoints\n\ncoordinates(pts.sample) =~ long + lat\nproj4string(pts.sample) <- CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\")\n\nnewPoints$grass.perc <- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100\n# Fit mod 1\nnewPoints$y_pred1 <- predict(m1, newPoints, type = 'response')\n\n# m1.pred <- st_as_sf(newPoints, coords = c('long', 'lat'), crs = st_crs(ks),\n#                     agr = 'constant') \n\nggplot()+\n\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = y_pred1))+\n    geom_sf(data = ks, fill = NA, color = 'black')+\n  scale_fill_viridis_c(values = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1))+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))+\n  facet_wrap(~year, ncol = 1)\n# Fit mod 2\nnewPoints$y_pred2 <- predict(m2, newPoints, type = 'response')\n\n\nggplot()+\n\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = y_pred2))+\n    geom_sf(data = ks, fill = NA, color = 'black')+\n  scale_fill_viridis_c()+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))+\n  facet_wrap(~year, ncol = 1)\n# Fit mod 3\nnewPoints$y_pred3 <- predict(m3, newPoints, type = 'response')\n\n# m3.pred <- st_as_sf(newPoints, coords = c('long', 'lat'), crs = st_crs(ks),\n                    # agr = 'constant') \n\nggplot()+\n\n  geom_tile(data = newPoints, aes(x = long, y = lat, fill = y_pred3))+\n    geom_sf(data = ks, fill = NA, color = 'black')+\n  scale_fill_viridis_c(values = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1))+\n  labs(x = 'Longitude', y = 'Latitude')+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 30))+\n  facet_wrap(~year, ncol = 1)"},{"path":"activity-3.html","id":"for-the-three-models-you-fit-in-question-3-which-model-makes-the-most-accurate-predictions-how-good-is-the-best-model-in-real-world-terms-remember-we-are-trying-to-predict-the-number-of-english-grain-aphids-which-is-a-count","chapter":"5 Activity 3","heading":"5.7 For the three models you fit in question #3, which model makes the most accurate predictions? How good is the best model in real world terms? Remember we are trying to predict the number of English grain aphids, which is a count!","text":"","code":"\nrmse.m1 <- rmse(df$EGA, as.numeric(predict(m1, df, type = 'response')))\nmae.m1 <- mae(df$EGA, as.numeric(predict(m1, df, type = 'response')))\n\nrmse.m2 <- rmse(df$EGA, as.numeric(predict(m2, df, type = 'response')))\nmae.m2 <- mae(df$EGA, as.numeric(predict(m2, df, type = 'response')))\n\nrmse.m3 <- rmse(df$EGA, as.numeric(predict(m3, df, type = 'response')))\nmae.m3 <- mae(df$EGA, as.numeric(predict(m3, df, type = 'response')))\n\n#m1 metrics\n\nm1Metrics <- df %>% \n  ggplot()+\n  geom_point(aes(predict(m1, df, type = 'response'), EGA), \nfill = 'purple4', color = 'black', shape = 21, size = 2,\n             alpha = .7)+\n  geom_abline(slope = 1)+\n  scale_y_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+\n  scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+  theme_bw()+\n  labs(title = 'm1', x = 'Predicted', y = 'Observed')+\n  annotate('text', label = paste0('RMSE: ', round(rmse.m1,1)), x = 700, y = 200)+\n  annotate('text', label = paste0('MAE: ', round(mae.m1,1)), x = 700, y = 100)+\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1,\n        text = element_text(size = 12)\n        )\n\n#m2 metrics\n\nm2Metrics <- df %>% \n  ggplot()+\n  geom_point(aes(predict(m2, df, type = 'response'), EGA), \n             fill = 'purple4', color = 'black', shape = 21, size = 2,\n             alpha = .7)+\n  geom_abline(slope = 1)+\n  scale_y_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+\n  scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+  theme_bw()+\n  labs(title = 'm2', x = 'Predicted', y = 'Observed')+\n  annotate('text', label = paste0('RMSE: ', round(rmse.m2,1)), x = 700, y = 200)+\n  annotate('text', label = paste0('MAE: ', round(mae.m2,1)), x = 700, y = 100)+\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1,\n        text = element_text(size = 12)\n        )\n\nm3Metrics <- df %>% \n  ggplot()+\n  geom_point(aes(predict(m3, df, type = 'response'), EGA), \n             fill = 'purple4', color = 'black', shape = 21, size = 2,\n             alpha = .7)+\n  geom_abline(slope = 1)+\n  scale_y_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+\n  scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000, 200))+\n  theme_bw()+\n  labs(title = 'm3', x = 'Predicted', y = 'Observed')+\n  annotate('text', label = paste0('RMSE: ', round(rmse.m3,1)), x = 700, y = 200)+\n  annotate('text', label = paste0('MAE: ', round(mae.m3,1)), x = 700, y = 100)+\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1,\n        text = element_text(size = 12)\n        )\n\nggarrange(m1Metrics,m2Metrics, m3Metrics, nrow = 1)\n\nAIC(m1, m2, m3)\n#>          df       AIC\n#> m1 34.97341 10896.425\n#> m2 13.90150  1913.024\n#> m3 50.09095 10464.469"},{"path":"activity-3.html","id":"summarize-your-results-using-words-numerical-values-and-figuresmaps.","chapter":"5 Activity 3","heading":"5.8 Summarize your results using words, numerical values and figures/maps.","text":"","code":""}]
